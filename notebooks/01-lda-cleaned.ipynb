{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preface\n",
    "This is some basic config stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from topfin.util import do_spacy_stuff, load_spacy\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "do_spacy_stuff()\n",
    "docs = load_spacy('data/spacy2.bin')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This following code is taken from [here](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py).\n",
    "It is used to  make these nice topic analysis plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx + 1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "These are parameters for the LDA algorithm, to make sure that the topic distribution looks\n",
    "somewhat sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 1000  # Max words to consider\n",
    "max_df = 0.60  # Consider words that only appear in at most 60% of the docs.\n",
    "min_df = 5  # Consider words that appear at least 5 times across all documents.\n",
    "n_topics = 10  # number of topics\n",
    "n_top_words = 10  # number of words to display per topic\n",
    "max_iter = 5  # How long to train for\n",
    "texts_won = [' '.join(t.lemma_ for t in d) for d in docs if d._.won]\n",
    "texts_lost = [' '.join(t.lemma_ for t in d) for d in docs if not d._.won]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LDA\n",
    "This is the code for the actual LDA algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def do_lda(texts, max_df, min_df, n_features, n_topics, max_iter, idf=False):\n",
    "    if idf:\n",
    "        tf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                        max_features=n_features,\n",
    "                                        stop_words='english')\n",
    "\n",
    "    else:\n",
    "        tf_vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                        max_features=n_features,\n",
    "                                        stop_words='english')\n",
    "\n",
    "    tf_matrix = tf_vectorizer.fit_transform(texts)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=max_iter,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=30.,\n",
    "                                    random_state=0)\n",
    "    lda.fit(tf_matrix)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    return lda, tf_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is the result of applying LDA on won topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lda, words = do_lda(texts_won, max_df, min_df, n_features, n_topics, max_iter, idf=True)\n",
    "plot_top_words(lda, words, n_top_words, 'LDA on won contracts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is the result of applying LDA on lost topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lda, words = do_lda(texts_lost, max_df, min_df, n_features, n_topics, max_iter, idf=True)\n",
    "plot_top_words(lda, words, n_top_words, 'LDA on lost contracts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see a slight difference in topics. Won contracts seem to mention topics related to golf,\n",
    "water and fishing and local community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Word-level analysis\n",
    "\n",
    "This is some pre-processing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "exclude = {\"CARDINAL\", \"ORDINAL\", \"PERCENT\", \"TIME\", \"QUANTITY\", \"DATE\"}\n",
    "\n",
    "ners_won = ['|'.join(str(e) for e in d.ents if e.label_ not in exclude) for d in docs if d._.won]\n",
    "ners_lost = ['|'.join(str(e) for e in d.ents if e.label_ not in exclude) for d in docs if not d._.won]\n",
    "\n",
    "\n",
    "def my_tokenizer(text):\n",
    "    return re.split(\"\\|\", text)\n",
    "\n",
    "\n",
    "def do_lda_on_ner(texts, max_df, min_df, n_features, n_topics, max_iter, idf=False):\n",
    "    if idf:\n",
    "        tf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                        max_features=n_features,\n",
    "                                        tokenizer=my_tokenizer)\n",
    "\n",
    "    else:\n",
    "        tf_vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                        max_features=n_features,\n",
    "                                        tokenizer=my_tokenizer)\n",
    "\n",
    "    tf_matrix = tf_vectorizer.fit_transform(texts)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=max_iter,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=30.,\n",
    "                                    random_state=0)\n",
    "    lda.fit(tf_matrix)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    return lda, tf_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "What follows, is an application of the LDA algorithm when only considering named entities.\n",
    "The results are rather inconclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lda, words = do_lda_on_ner(ners_won, 1.0, 5, 100, 3, max_iter, idf=False)\n",
    "plot_top_words(lda, words, n_top_words, 'Won contracts (NER)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lda, words = do_lda_on_ner(ners_lost, 1.0, 5, 100, 3, max_iter, idf=False)\n",
    "plot_top_words(lda, words, n_top_words, 'Lost contracts (NER)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "More processing/visualisation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_barchart_won_lost(labels_won, labels_lost, vals_won, vals_lost, title):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "    axes.flatten()\n",
    "    axes[0].barh(labels_won, vals_won)\n",
    "    #range(len(labels_won)), vals_won)\n",
    "    #axes[0].yticks(ticks=range(len(labels)), labels=labels, )\n",
    "    axes[0].set_title(\"won\")\n",
    "    axes[1].barh(labels_lost, vals_lost)\n",
    "    # range(len(labels_lost)), vals_lost)\n",
    "    #axes[1].yticks(ticks=range(len(labels_lost)), labels=labels, )\n",
    "    axes[1].set_title(\"lost\")\n",
    "    # plt.barh(range(len(labels)), vals)\n",
    "    # plt.ylabel(\"Labels\")\n",
    "    # plt.xlabel(\"Number of Docs mentioning the word\")\n",
    "    # plt.yticks(ticks=range(len(labels)), labels=labels, )\n",
    "    fig.suptitle(title)\n",
    "    #plt.hist(top_20)\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.01)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normalise(d: dict, ratio):\n",
    "    return Counter({k: v * ratio for k, v in d.items()})\n",
    "\n",
    "\n",
    "won_lost_ratio = sum(1 for d in docs if d._.won) / sum(1 for d in docs if not d._.won)\n",
    "assert sum(1 for d in docs if not d._.won) * won_lost_ratio == sum(1 for d in docs if d._.won)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_most_common(doc_repr, n_features=1000, df_ratio=1.):\n",
    "    tf_vectorizer = CountVectorizer(max_features=n_features, tokenizer=my_tokenizer, max_df=df_ratio)\n",
    "    tf_counts = tf_vectorizer.fit_transform(doc_repr).toarray()\n",
    "    ctr = Counter(dict(zip(tf_vectorizer.get_feature_names(), np.sum(tf_counts, axis=0))))\n",
    "    return ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "What follows here, is a word class analysis preceded by named entity analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ctr_won = get_most_common(ners_won)\n",
    "labels, vals = zip(*ctr_won.most_common(20))\n",
    "\n",
    "ctr_lost = get_most_common(ners_lost)\n",
    "labels_lost, vals_lost = zip(*normalise(ctr_lost, won_lost_ratio).most_common(20))\n",
    "print_barchart_won_lost(labels, labels_lost, vals, vals_lost, \"entities in contracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "adjs_won = ['|'.join(str(e.lemma_) for e in d if e.pos_ == 'ADJ' and str(e) != '-') for d in docs if d._.won]\n",
    "adjs_lost = ['|'.join(str(e.lemma_) for e in d if e.pos_ == 'ADJ' and str(e) != '-') for d in docs if not d._.won]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ctr_adjs_won = get_most_common(adjs_won, df_ratio=0.5)\n",
    "labels, vals = zip(*ctr_adjs_won.most_common(20))\n",
    "\n",
    "ctr_adjs_lost = get_most_common(adjs_lost, df_ratio=0.5)\n",
    "labels_lost, vals_lost = zip(*normalise(ctr_adjs_lost, won_lost_ratio).most_common(20))\n",
    "print_barchart_won_lost(labels, labels_lost, vals, vals_lost, \"adjectives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "verbs_won = ['|'.join(str(e.lemma_) for e in d if e.pos_ == 'VERB' and str(e) != '-') for d in docs if d._.won]\n",
    "verbs_lost = ['|'.join(str(e.lemma_) for e in d if e.pos_ == 'VERB' and str(e) != '-') for d in docs if not d._.won]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ctr_verbs_won = get_most_common(verbs_won, df_ratio=0.6)\n",
    "labels, vals = zip(*ctr_verbs_won.most_common(20))\n",
    "\n",
    "ctr_verbs_lost = get_most_common(verbs_lost, df_ratio=0.6)\n",
    "labels_lost, vals_lost = zip(*normalise(ctr_verbs_lost, won_lost_ratio).most_common(20))\n",
    "print_barchart_won_lost(labels, labels_lost, vals, vals_lost, \"verbs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "adverbs_won = ['|'.join(str(e.lemma_) for e in d if e.pos_ == 'ADV' and str(e) != '-') for d in docs if d._.won]\n",
    "adverbs_lost = ['|'.join(str(e.lemma_) for e in d if e.pos_ == 'ADV' and str(e) != '-') for d in docs if not d._.won]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ctr_adverbs_won = get_most_common(adverbs_won, df_ratio=.3)\n",
    "labels, vals = zip(*ctr_adverbs_won.most_common(20))\n",
    "\n",
    "ctr_adverbs_lost = get_most_common(adverbs_lost, df_ratio=.3)\n",
    "labels_lost, vals_lost = zip(*normalise(ctr_adverbs_lost, won_lost_ratio).most_common(20))\n",
    "print_barchart_won_lost(labels, labels_lost, vals, vals_lost, \"verbs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lengths\n",
    "\n",
    "We see a that won contracts seem to be slightly longer on average.\n",
    "However, as mentioned before this might be an artefact of the form, as latter forms contain more\n",
    "fields to fill with content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "lengths_won = [len(d) for d in docs if d._.won]\n",
    "lengths_lost = [len(d) for d in docs if not d._.won]\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10), sharey=True, sharex=True)\n",
    "axes.flatten()\n",
    "axes[0].hist(lengths_won, bins=50)\n",
    "axes[0].set_title(\"# of Token distribution for won contracts\")\n",
    "axes[1].hist(lengths_lost, bins=50, weights=[won_lost_ratio] * len(lengths_lost))\n",
    "axes[1].set_title(\"# of Token distribution for lost contracts\")\n",
    "\n",
    "plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "noun_chunks_won = ['|'.join(str(e) for e in d.noun_chunks if str(e).lower().strip() not in stopwords) for d in docs if\n",
    "                   d._.won]\n",
    "noun_chunks_lost = ['|'.join(str(e) for e in d.noun_chunks if str(e).lower().strip() not in stopwords) for d in docs if\n",
    "                    not d._.won]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ctr_np_won = get_most_common(noun_chunks_won, df_ratio=0.3)\n",
    "labels, vals = zip(*ctr_np_won.most_common(20))\n",
    "\n",
    "ctr_np_lost = get_most_common(noun_chunks_lost, df_ratio=0.3)\n",
    "labels_lost, vals_lost = zip(*normalise(ctr_np_lost, won_lost_ratio).most_common(20))\n",
    "print_barchart_won_lost(labels, labels_lost, vals, vals_lost, \"Noun Phrases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "max_df = 0.60\n",
    "min_df = 5\n",
    "n_topics = 10\n",
    "n_top_words = 10\n",
    "max_iter = 5\n",
    "lda, words = do_lda(texts_won + texts_lost, max_df, min_df, n_features, n_topics, max_iter, idf=True)\n",
    "plot_top_words(lda, words, n_top_words, 'LDA on joint contracts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LDA on won/lost contracts (jointly)\n",
    "Here we model the topics jointly on won/lost contracts and observe whether won/lost contracts\n",
    "differ in topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wons_features = CountVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english').fit_transform(texts_won)\n",
    "losts_features = CountVectorizer(max_df=max_df, min_df=min_df,\n",
    "                                 max_features=n_features,\n",
    "                                 stop_words='english').fit_transform(texts_lost)\n",
    "topics_won = lda.transform(wons_features)\n",
    "topics_lost = lda.transform(losts_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "average_won = np.average(topics_won, axis=0)\n",
    "average_lost = np.average(topics_lost, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(10), average_won, label=\"Wons\")\n",
    "plt.plot(range(10), average_lost, label=\"Losts\")\n",
    "plt.xticks(range(10))\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "summed_won = np.sum(topics_won, axis=0)\n",
    "summed_lost = np.sum(topics_lost, axis=0) * won_lost_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(10), summed_won, label=\"Wons\")\n",
    "plt.plot(range(10), summed_lost, label=\"Losts\")\n",
    "plt.xticks(range(10))\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "def get_ngrams(docs, n_features, df_ratio, ngrams=2):\n",
    "    tf_vectorizer = CountVectorizer(max_features=n_features, max_df=df_ratio, ngram_range=(ngrams, ngrams),\n",
    "                                    stop_words=nltk.corpus.stopwords.words('english'))\n",
    "    tf_counts = tf_vectorizer.fit_transform(docs).toarray()\n",
    "    ctr = Counter(dict(zip(tf_vectorizer.get_feature_names(), np.sum(tf_counts, axis=0))))\n",
    "    return ctr, tf_counts, tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stuff = 'non confidential information include as much information as possible on the request description of sought after technology/information features field of use'\n",
    "\n",
    "docs_won = [str(d).split('BACKGROUND')[-1].replace(stuff, '') for d in docs if d._.won]\n",
    "\n",
    "docs_lost = [str(d).split('BACKGROUND')[-1].replace(stuff, '') for d in docs if not d._.won]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ctr_np_won, *_ = get_ngrams(docs_won, 1000, df_ratio=0.3)\n",
    "labels, vals = zip(*ctr_np_won.most_common(20))\n",
    "\n",
    "ctr_np_lost, *_ = get_ngrams(docs_lost, 1000, df_ratio=0.3)\n",
    "labels_lost, vals_lost = zip(*normalise(ctr_np_lost, won_lost_ratio).most_common(20))\n",
    "print_barchart_won_lost(labels, labels_lost, vals, vals_lost, \"Bigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ctr_np_won, *_ = get_ngrams(docs_won, 1000, df_ratio=0.3, ngrams=3)\n",
    "labels, vals = zip(*ctr_np_won.most_common(20))\n",
    "\n",
    "ctr_np_lost, *_ = get_ngrams(docs_lost, 1000, df_ratio=0.3, ngrams=3)\n",
    "labels_lost, vals_lost = zip(*normalise(ctr_np_lost, won_lost_ratio).most_common(20))\n",
    "print_barchart_won_lost(labels, labels_lost, vals, vals_lost, \"Trigrams\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ctr_np_won, *_ = get_ngrams(docs_won, 1000, df_ratio=0.3, ngrams=4)\n",
    "labels, vals = zip(*ctr_np_won.most_common(20))\n",
    "\n",
    "ctr_np_lost, *_ = get_ngrams(docs_lost, 1000, df_ratio=0.3, ngrams=4)\n",
    "labels_lost, vals_lost = zip(*normalise(ctr_np_lost, won_lost_ratio).most_common(20))\n",
    "print_barchart_won_lost(labels, labels_lost, vals, vals_lost, \"4-grams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "\n",
    "def pmi(x, y, joint, totals):\n",
    "    p_xy = joint / totals\n",
    "    p_x = x / totals\n",
    "    p_y = y / totals\n",
    "    return log(p_xy / (p_x * p_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ctr_ngrams, tf_counts, feature_names = get_ngrams(docs_won + docs_lost, 1000, df_ratio=0.3, ngrams=4)\n",
    "print(tf_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "most_common = set(x for x, _ in ctr_ngrams.most_common(20))\n",
    "assert len(tf_counts.T) == len(feature_names)\n",
    "total = len(docs_won + docs_lost)\n",
    "y_won = len(docs_won)\n",
    "y_lost = len(docs_lost)\n",
    "for counts, features in zip(tf_counts.T, feature_names):\n",
    "    assert len(counts) == total\n",
    "    if features in most_common:\n",
    "        counts = counts.clip(0, 1)\n",
    "        x = sum(counts)\n",
    "        xy_won = sum(counts[:y_won])\n",
    "        xy_lost = sum(counts[y_won:])\n",
    "        print(f\"For `{features}`. PMI won: {pmi(x, y_won, xy_won, total):.3f}. {pmi(x, y_lost, xy_lost, total):.3f}\")\n",
    "#     print(f\"For word {k}:\")\n",
    "#     print(f\"PMI won: {pmi()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}